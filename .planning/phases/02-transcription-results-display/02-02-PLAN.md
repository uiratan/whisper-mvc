---
phase: 02-transcription-results-display
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/components/AudioUploader.tsx
autonomous: false

must_haves:
  truths:
    - "User sees transcription text after upload completes"
    - "User sees timestamps for each transcription segment"
    - "User sees error message if transcription fails"
    - "Transcription results are clearly formatted and readable"
  artifacts:
    - path: "src/components/AudioUploader.tsx"
      provides: "Transcription results display with timestamps"
      min_lines: 350
      contains: "TranscriptionResult"
  key_links:
    - from: "src/components/AudioUploader.tsx"
      to: "API response transcription field"
      via: "UploadResponse interface"
      pattern: "transcription.*TranscriptionResult"
    - from: "TranscriptionDisplay component"
      to: "transcription.segments"
      via: "map over segments array"
      pattern: "segments.*map"
---

<objective>
Display transcription results with timestamps in the frontend after audio processing completes.

Purpose: Allow users to see the transcribed text with word-level timestamps from whisper.cpp, completing the core value proposition of the application.

Output: Updated AudioUploader component that displays transcription results in a readable format with timestamps for each segment.
</objective>

<execution_context>
@/home/uira/.claude/get-shit-done/workflows/execute-plan.md
@/home/uira/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@src/components/AudioUploader.tsx

## From Phase 1 & Plan 02-01

AudioUploader component currently:
- Handles file selection and validation
- Shows upload progress bar
- Displays status messages (success/error/idle)
- Uploads to /api/upload endpoint via XMLHttpRequest

Plan 02-01 modified /api/upload to return:
```typescript
{
  success: boolean
  fileName: string
  filePath: string
  fileSize: number
  message: string
  transcription: {
    text: string
    segments: Array<{
      start: number  // seconds
      end: number    // seconds
      text: string
    }>
  }
}
```

## Design Requirements

**Display transcription prominently after successful upload.**

**Timestamp format**: Convert seconds to MM:SS format for readability (e.g., 0:05 - 0:12).

**Layout**:
- Full transcription text at top (paragraph style)
- Timestamped segments below in list/table format
- Each segment shows: [start-end] text

**Error handling**: If transcription field is missing or empty, show appropriate message.

**UI patterns**: Follow existing Tailwind styling from AudioUploader (card layout, indigo theme, clean spacing).
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update types and state to handle transcription data</name>
  <files>
    src/components/AudioUploader.tsx
  </files>
  <action>
Update UploadResponse interface to include transcription field:

```typescript
interface TranscriptionSegment {
  start: number
  end: number
  text: string
}

interface TranscriptionResult {
  text: string
  segments: TranscriptionSegment[]
}

interface UploadResponse {
  success: boolean
  fileName?: string
  filePath?: string
  fileSize?: number
  message: string
  transcription?: TranscriptionResult
}
```

Add new state variable to store transcription results:

```typescript
const [transcription, setTranscription] = useState<TranscriptionResult | null>(null)
```

Add this state declaration after the existing state variables (after statusType).

In handleUpload, when upload succeeds (xhr.status === 200), extract and store transcription:

```typescript
xhr.addEventListener('load', () => {
  if (xhr.status === 200) {
    const response: UploadResponse = JSON.parse(xhr.responseText)
    setStatusMessage(response.message || 'Upload successful!')
    setStatusType('success')
    setUploadProgress(100)

    // Store transcription if present
    if (response.transcription) {
      setTranscription(response.transcription)
    }
  } else {
    // ... existing error handling
  }
  setIsUploading(false)
})
```

In handleClear function, also clear transcription state:

```typescript
const handleClear = () => {
  setSelectedFile(null)
  setUploadProgress(0)
  setStatusMessage('')
  setStatusType('idle')
  setTranscription(null)  // Add this line
  // Reset file input
  const fileInput = document.getElementById('audio-file-input') as HTMLInputElement
  if (fileInput) fileInput.value = ''
}
```
  </action>
  <verify>
Run TypeScript compilation:
```bash
npx tsc --noEmit
```

Check that transcription state and types are added:
```bash
grep "TranscriptionResult" src/components/AudioUploader.tsx
grep "useState.*transcription" src/components/AudioUploader.tsx
```
  </verify>
  <done>
UploadResponse interface includes optional transcription field with TranscriptionResult type. Component state includes transcription variable. Upload handler stores transcription from API response. Clear handler resets transcription state.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create TranscriptionDisplay component section</name>
  <files>
    src/components/AudioUploader.tsx
  </files>
  <action>
Add helper function to format timestamp (seconds to MM:SS):

```typescript
const formatTimestamp = (seconds: number): string => {
  const mins = Math.floor(seconds / 60)
  const secs = Math.floor(seconds % 60)
  return `${mins}:${secs.toString().padStart(2, '0')}`
}
```

Place this helper function after formatFileSize, before the return statement.

Add transcription display section in the JSX, after the Upload Button and before the closing </div>:

```tsx
{/* Transcription Results */}
{transcription && (
  <div className="mt-8 border-t border-gray-200 pt-8">
    <h3 className="text-xl font-semibold text-gray-800 mb-4">Transcription</h3>

    {/* Full Text */}
    <div className="mb-6 p-4 bg-gray-50 rounded-lg border border-gray-200">
      <h4 className="text-sm font-medium text-gray-700 mb-2">Full Text</h4>
      <p className="text-gray-900 leading-relaxed">{transcription.text}</p>
    </div>

    {/* Timestamped Segments */}
    {transcription.segments.length > 0 && (
      <div>
        <h4 className="text-sm font-medium text-gray-700 mb-3">Timestamped Segments</h4>
        <div className="space-y-2">
          {transcription.segments.map((segment, index) => (
            <div
              key={index}
              className="flex gap-4 p-3 bg-white border border-gray-200 rounded-lg hover:bg-indigo-50 hover:border-indigo-300 transition-colors"
            >
              <div className="flex-shrink-0 text-sm font-mono text-indigo-600 font-medium">
                [{formatTimestamp(segment.start)} - {formatTimestamp(segment.end)}]
              </div>
              <div className="flex-1 text-gray-800">
                {segment.text}
              </div>
            </div>
          ))}
        </div>
      </div>
    )}
  </div>
)}
```

This section only appears when transcription data exists. It shows:
1. Full transcribed text in a gray box
2. Individual segments with timestamps in [MM:SS - MM:SS] format
3. Hover effects on segments for better UX

If transcription.text exists but segments array is empty, only the full text section displays.
  </action>
  <verify>
Run TypeScript compilation:
```bash
npx tsc --noEmit
```

Verify transcription display markup is present:
```bash
grep -n "Transcription Results" src/components/AudioUploader.tsx
grep -n "formatTimestamp" src/components/AudioUploader.tsx
grep -n "segments.map" src/components/AudioUploader.tsx
```
  </verify>
  <done>
AudioUploader component displays transcription results when available. Full text shown in styled box. Timestamped segments displayed in list format with MM:SS timestamps. Conditional rendering ensures display only appears after successful transcription.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete whisper.cpp transcription flow from audio upload to timestamped display.

Backend integration (Plan 02-01):
- Audio files converted to WAV 16kHz mono format
- whisper.cpp invoked via spawn with base model
- Transcription parsed and returned with timestamps

Frontend display (Plan 02-02):
- AudioUploader shows transcription after upload
- Full text and timestamped segments displayed
- Clean Tailwind styling with hover effects
  </what-built>
  <how-to-verify>
1. Start development server:
```bash
cd /home/uira/git/ai/whisper-test
npm run dev
```

2. Open browser to http://localhost:3000

3. Test the complete flow:
   - Select an audio file (.wav, .mp3, or .ogg)
   - Click "Upload File" button
   - Observe upload progress bar (should reach 100%)
   - Wait for transcription processing (whisper.cpp running)
   - Verify transcription results appear below upload button

4. Check transcription display:
   - "Full Text" section shows complete transcription
   - "Timestamped Segments" shows each segment with [MM:SS - MM:SS] format
   - Timestamps are accurate and sequential
   - Hover over segments shows indigo highlight effect

5. Test error handling:
   - If whisper.cpp binary not found, should see clear error message
   - If model file missing, should see descriptive error
   - Upload invalid file type, verify rejection

6. Test clear functionality:
   - After successful transcription, click clear (X button)
   - Verify transcription results disappear
   - Verify can upload new file

Expected behavior:
- Upload completes and shows success message
- Transcription appears automatically (no page refresh needed)
- Timestamps formatted as minutes:seconds (e.g., 0:05 - 0:12)
- Text is readable and properly formatted
- No console errors in browser dev tools

Requirements validated:
- TRNS-01: Backend invokes whisper.cpp ✓
- TRNS-02: Audio converted to WAV 16kHz mono ✓
- TRNS-03: Transcription returned with timestamps ✓
- DISP-01: Page displays transcription with timestamps ✓
  </how-to-verify>
  <resume-signal>
Type "approved" if transcription flow works end-to-end, or describe any issues encountered (error messages, missing timestamps, formatting problems, etc.)
  </resume-signal>
</task>

</tasks>

<verification>
After completing all tasks:

1. TypeScript compilation passes:
```bash
npx tsc --noEmit
```

2. Component includes transcription display code:
```bash
grep "transcription &&" src/components/AudioUploader.tsx
grep "Timestamped Segments" src/components/AudioUploader.tsx
```

3. Types properly defined:
```bash
grep "interface TranscriptionResult" src/components/AudioUploader.tsx
```

4. State management includes transcription:
```bash
grep "useState.*TranscriptionResult" src/components/AudioUploader.tsx
```
</verification>

<success_criteria>
1. AudioUploader component updated with transcription result types
2. Transcription data stored in component state after successful upload
3. Full transcription text displayed in styled section
4. Timestamped segments rendered in list format with MM:SS timestamps
5. Clear button resets transcription display
6. Human verification confirms end-to-end flow works correctly
</success_criteria>

<output>
After completion, create `.planning/phases/02-transcription-results-display/02-02-SUMMARY.md`
</output>
